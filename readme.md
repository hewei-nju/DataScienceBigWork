# README:speech_balloon:

#### 0. 前言:o:

0. 这是我们**数据科学基础大作业**的私有仓库，当你们看到了这个文件时，就证明你们可以随意地访问这个**私有仓库**地内容了。

1. 可能会使用到的一些`git`:hibiscus:命令：

    ```wiki
    $ git pull	对你电脑本地仓库进行内容的更新，也就是把远端仓库新的内容拉到你的仓库
    $ git log	查看这个仓库的提交记录
    $ git status 查看项目当前的状态
    $ pwd		 返回当前路径
    $ dir 	 	 查看当前目录下文件/文件夹的名称
    $ cd ../     从当前目录下倒退一个目录
    $ cd FileFolderName 从当前目录进入子目录
    $ cat FileName 该目录下文件名为FileName的文件内容
    ```

2. 我们会有至少两份不同的代码，也就是有两个`pycharm`仓库，这两个仓库先不需要合并，因为里面的逻辑差异很大，但是我们可以时不时的去看下小组内成员的代码逻辑等等，然后copy有用的东西，:triangular_flag_on_post:同时也是一个督促自己完成既定工作的方法。

3. 注意在自己的`project`里面添加一个专属的:blush:README，用来记录你每天的工作内容，以及在那些地方进行了修改，这个到时候也能在报告中添加进去:watermelon:。

4. 我们会在项目进行的中期，进行一次`Code Review`，将代码等进行一次整合，然后继续对应的工作。

#### 1. 项目主方向和流程:arrow_forward:

1. 项目主题：重大突发公共卫生事件下的网络社会心态及公众情绪引导研究
2. 项目的主方向：在刻画新型冠状病毒(COVID-19)传播这一重大公共卫生事件情景下，以大数据技术深描中国大众的网络社会心态。

3. 项目的主要流程：

    1. 下载新闻文本，按照一定格式保存至txt文档(暂时确定是不同的文本格式)，一天的新闻一个路径
    2. 对所有新闻文本进行时间轴分类；**先分层**，然后在每层中通过**随机函数随机抽取**样本新闻（总共占新闻总量的10%），人工打上**心态标签词**，人工定义心态词典。
    3. 大数据分析
    4. 词频分析（计算TF-IDF），对于每个新闻提取出现频率最高的心态词，判断新闻的主题思想。
    5. 机器学习
        利用1000条新闻样本，以及人工给定的label 训练出一个归类模型 (classification)
        利用归类机器模型，将剩下9000条新闻归类。
        在9000条里选1000条作为validation data set，人工给出label作为ground truth。然后通过比较机器给的label和人工label，来计算机器模型的accuracy (这个是机器学习中很重要的一个环节，模型效果好不好，要在validation data set上验证)。
    6. Temporal and demographic analysis
        Temporal 比较容易你们slides有，新闻应该是反映病毒扩散 ---> 防疫资源紧缺 ---> 物资配给 ---> 有序复工
        另外你们可以通过你们的分析看看社会心态的变化，紧张 ---> 担忧 ---> 恐慌---> 万众一心 ---> 感动 --->希望 ---> 喜悦
        demographic 上面，应该重点关注武汉、湖北 和其他地域的区别

    * 写总结报告。

#### 2. 对新闻文本的分类（按照重要事件以时间轴分类）:bookmark:

* 不重视与无奈扩散阶段

```wiki
2019.12.08 - 2020.01.22
这个阶段标志性事件为2019.12.08发现首例肺炎患者和2020.01.22湖北启动公共卫生事件二级响应：
1. 期间央媒的新闻报导内容（包括新闻标题和新闻内容）
2. 期间较大影响力的新闻报导内容（包括新闻标题和新闻内容）
3. 期间重点新闻的评论情况（需要定义重点新闻，获取首发新闻平台的评论内容）
```

* 资源缺乏阶段

```wiki
2020.01.23 - 2020.02.07
标志性事件为2020.01.23武汉宣布“封城”和2020.02.07“吹哨人”李文亮去世：
1. 期间央媒的新闻报导内容（包括新闻标题和新闻内容）
2. 期间较大影响力的新闻报导内容（包括新闻标题和新闻内容）
3. 期间重点新闻的评论情况（需要定义重点新闻，获取首发新闻平台的评论内容）
```

* 严格统一管控和物资配给阶段

```wiki
2020.02.10 - 2020.02.13
标志性事件是2020.02.10，19省对口支援湖北武汉外16个市州及县级市和2020.02.13湖北省相关领导的变动：
1. 期间央媒的新闻报导内容（包括新闻标题和新闻内容）
2. 期间较大影响力的新闻报导内容（包括新闻标题和新闻内容）
3. 期间重点新闻的评论情况（需要定义重点新闻，获取首发新闻平台的评论内容）
```

* 有序复工阶段

```wiki
2020.03.10 - 2020.06.20
标志性事件是各省开始有序复工复产：
1. 期间央媒的新闻报导内容（包括新闻标题和新闻内容）
2. 期间较大影响力的新闻报导内容（包括新闻标题和新闻内容）
3. 期间重点新闻的评论情况（需要定义重点新闻，获取首发新闻平台的评论内容）
```

#### 3. 数据源:link:

* 百度新闻API： https://news.baidu.com/?cmd=1&class=reci
* 新浪新闻API： https://news.sina.com.cn/roll/#pageid=153&lid=2509&k=&num=50&page=1 
* 天涯API：http://bbs.tianya.cn/list.jsp?item=funinfo&grade=3&order=1 
* 荔枝API：http://news.jstv.com/ 
* 新华API： http://qc.wa.news.cn/nodeart/list?nid=11147664&pgnum=1&cnt=10&tp=1&orderby=1 

#### 4. 数据分析:red_circle:

![image-20201128215946260](C:\Users\hewei\AppData\Roaming\Typora\typora-user-images\image-20201128215946260.png)

* 心态字典
* TF-IDF
* 聚类
* 新闻标签
* 语义分析

#### 5. 项目进展:beginner:

`markdown语法中，":heavy_check_mark:"对应下列绿色的√，":x:"对应红色的x`

---- Time ----	---- Sign ----	----Main Work----

* 2020-11-27：
  
    *  :heavy_check_mark:初步实现了对新浪新闻和人民日报新闻的爬取
    * :x:对于数据的格式没能达成共识
* 2020-11-28：

    * :heavy_check_mark:小组讨论结果很不错，至少是有稳定的沟通和联系，能够去探讨如何解决问题。然后初步探讨了下主方向。
    * :x:下载了相关数据，但是不可用，所以具体工作几乎没有进展。
* 2020-11-29：

    * :heavy_check_mark:初步过了一遍如何写比较不错的爬虫，然后将原来的代码完全的删了，重新写了一份。总体来说很不错，也能够下载相关数据了，重点是编码风格完全不同，可读性好多了。
    * :heavy_check_mark:进行了一次小组的周会，各自谈了谈自己遇到的问题以及一些疑惑，初步解决了相关疑惑。棒棒的！！！:airplane:
    * :x:不过数据仍然没有下载下来。
* 2020-11-30：

    * :heavy_check_mark::今天将所有的筛选新闻的Key Words​进行了merge，并晒出了重复的，然后初略的排了下在新闻中可能出现的先后次序。
    * :x:数据！数据！数据！没有下载下来！！！
* 2020-12-01：

    * :heavy_check_mark:修改了多次代码，把`ewspaper.article.ArticleException: Article `download()` failed with HTTPSConnectionPool(host='news.sina.com.cn', port=443): Read timed out.`异常进行了订正！`StackOverflow`真的很不错，不过英文阅读太差了！！！

    * :heavy_check_mark:很开心的是:smile:，今天把**新浪新闻**的数据下载了下来。
    * :heavy_check_mark:将周末创建的GitHub私有仓库进行了开放，和队友们通过GitHub进行合作，同时也希望通过这次合作，加深对GitHub使用的了解，也让自己更熟悉进行一个小项目是怎样的流程！:bird:
    * :x:`​emmm!`不过，今天代码修正的时间太多了，出现了很多次问题。最重要的数据格式我还是理解错了，然后导致下载了的4个月的数据都没有用。啊:cry:!
* 2020-12-02：
    * :heavy_check_mark:很不错，两份爬虫的代码都能持续很长时间的工作，:smile_cat:。人民日报的新闻也都下载下来了。
    * :heavy_check_mark:队友都加入了GitHub，希望合作顺利，go go go！:black_flag:
    * :x:不过，新闻的随机分配函数，评论，以及心态词典的构建都还没能进行。
* 2020-12-03：
* :heavy_check_mark:pre，上课，写COA！！！
  
* :x:今天大作业没有任何进展，哭了！！！:cry:
* 2020-12-04:

  * :x:小组成员也都在肝作业！！！真得加快进度了，不然期末真不行！！！
* 2020-12-05:

  * :x:啊，我以为自己上课学懂了，结果做编码作业还是感觉怪怪的，还没助教来稍作指导和答疑！！！虽然菜，但我感觉不是我自己的原因！！！